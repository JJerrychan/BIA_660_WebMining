{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>An example of SNS</center>\n",
    "\n",
    "- Threadless.com is a crowdsouring website for graphic designs.\n",
    "- Desginers submit artworks and recieve ratings from the community within a seven-day period. \n",
    "- Designs with the best scores will be selected to print on T-shirts and other products for sale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping objectives\n",
    "\n",
    "- Get a sample of users and artifacts. Consider a sampling strategy. \n",
    "- Scrape artifact-level features.\n",
    "- Scrape user-level features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.threadless.com/designs/archive?page=1', 'https://www.threadless.com/designs/archive?page=2', 'https://www.threadless.com/designs/archive?page=3', 'https://www.threadless.com/designs/archive?page=4', 'https://www.threadless.com/designs/archive?page=5']\n"
     ]
    }
   ],
   "source": [
    "# Get five urls of pages as a sample of latest artifacts.\n",
    "\n",
    "link=\"https://www.threadless.com/designs/archive?page=\"\n",
    "num=list(range(1,6))\n",
    "pages=[]\n",
    "for i in num:\n",
    "    page=link+str(i)\n",
    "    pages.append(page)\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on page https://www.threadless.com/designs/archive?page=1\n",
      "working on page https://www.threadless.com/designs/archive?page=2\n",
      "working on page https://www.threadless.com/designs/archive?page=3\n",
      "working on page https://www.threadless.com/designs/archive?page=4\n",
      "working on page https://www.threadless.com/designs/archive?page=5\n"
     ]
    }
   ],
   "source": [
    "# Get urls of all the designs in these pages\n",
    "# To reduce the load to their server, will demonnstrate one page\n",
    "\n",
    "designs=[]\n",
    "for i in pages:\n",
    "    print('working on page'+str(' ')+str(i))\n",
    "    response=requests.get(i)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    links=soup.find('ol',class_='feed-archive th-grided')\n",
    "    li=links.find_all('li',class_=\"old\")\n",
    "    for j in li:\n",
    "        name=j.find(\"a\")[\"href\"]\n",
    "        designs.append(name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/designs/warning-alt-3',\n",
       " '/designs/unstoppable-9',\n",
       " '/designs/corporate-greed-2',\n",
       " '/designs/solar-lion',\n",
       " '/designs/goblincore-18']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "designs[:5]\n",
    "\n",
    "# can write out the sample of artifacts \n",
    "# with open('designs.csv', 'w') as csvfile:\n",
    "#    writer=csv.writer(csvfile, delimiter=',')\n",
    "#    writer.writerows(zip(designs))\n",
    "\n",
    "\n",
    "# read in your sample\n",
    "# raw_data_file = open(\"designs.csv\", 'r')\n",
    "# csv_data_file = csv.reader(raw_data_file, delimiter=',')\n",
    "# designs = []\n",
    "# for line in csv_data_file:\n",
    "#     print(line[0])\n",
    "#     designs.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Warning! Alt 3', 'ClosetPrints', '2.00', '1')\n",
      "('Un....Stoppable.', 'ArtofPig18', '2.80', '5')\n",
      "('Corporate Greed 2', 'ArtofPig18', '2.00', '6')\n",
      "('Solar Lion', 'MitsukiRising', '3.77', '26')\n",
      "('goblincore', 'muktiharjanto', '2.33', '6')\n",
      "('Brains!', 'SpaceDat120', '3.75', '4')\n",
      "('Halloween T-shirt', 'sunsetandchill', '2.00', '2')\n",
      "('Retro Atlantic blue ocean coast beach landscape in Portugal with 3D rhythmic concrete docking', 'EdyArtSpace', '1.60', '5')\n",
      "('Break Wall', 'i997', '1.00', '2')\n",
      "('Sinister Claus', 'gulayfather', '4.00', '3')\n",
      "('Forever Yours', 'Elaraslove', '2.00', '1')\n",
      "('Frog Witch', 'monstiker', '2.78', '9')\n",
      "(\"Bushido '89\", 'TheWestwood', '2.57', '7')\n",
      "('Game Over Graffity Style', 'bakhus', '1.00', '2')\n",
      "('WE ARE PEOPLE TO', 'CruzArtGallery', '3.00', '4')\n",
      "('boo donut', 'KTRK', '3.33', '3')\n",
      "('super mario x street fighter ken', 'KTRK', '3.00', '5')\n",
      "('Banner', 'CallyRaphael', '2.00', '1')\n",
      "('You Saw Nothing', 'Xentee', '2.50', '4')\n",
      "('Okay to decay', 'biernatt', '2.86', '7')\n",
      "('I am gaming', 'mhs23', '3.00', '2')\n",
      "('please', 'Turborat14', '3.43', '7')\n",
      "('To The Moon', 'santokie', '3.00', '18')\n",
      "('Demond Be Gone', 'moviereplicars', '2.83', '6')\n",
      "('Red panda', 'UnfoundedHope', '3.71', '7')\n",
      "('Poe', 'UnfoundedHope', '2.71', '7')\n",
      "('This is PIT 2', 'makerofstuff', '3.33', '9')\n",
      "('This is PIT 5', 'makerofstuff', '2.67', '9')\n",
      "('Proud', 'UnfoundedHope', '3.50', '4')\n",
      "('Love Cake 3', 'makerofstuff', '3.67', '3')\n",
      "('The mouse and the morel', 'UnfoundedHope', '2.64', '11')\n",
      "('Proud', 'UnfoundedHope', '1.00', '1')\n",
      "('Hands up', 'UnfoundedHope', '3.00', '2')\n",
      "('Born to be a Gamer', 'rteestyle', '2.57', '7')\n",
      "('The Sun Still Rise', 'gerberaloka', '3.36', '14')\n",
      "('Fantasy Neighborhood', 'PammyB', '2.78', '9')\n",
      "('Pixel Monster Guide', 'EmeraldMakes', '2.43', '7')\n",
      "('AWOOOOOLF!!', 'leeagosila', '3.15', '20')\n",
      "('Let me in!', 'SpaceDat120', '3.86', '7')\n",
      "('Cat Shroom!', 'Astrid13', '3.38', '34')\n",
      "('Monstrous Werewolf', 'SpaceDat120', '3.67', '6')\n",
      "('A Goblin Dreams', 'NoMotiveInc', '3.31', '29')\n",
      "('Mushrooms Botanical', 'kimprut', '3.47', '15')\n",
      "('Face monster', 'Gabriel0', '3.00', '2')\n",
      "('Period Time', 'churrumiaus', '3.54', '13')\n",
      "('Wolf globe', 'dragongrr', '1.50', '2')\n",
      "('Warning! Alt 2', 'ClosetPrints', '1.50', '2')\n",
      "('Warning! Alt 1', 'ClosetPrints', '1.50', '2')\n",
      "('Draw Me Like One Of Your French Girls Cat Funny T-shirt', 'tobefonseca', '3.50', '4')\n",
      "('9 Lives Cat Halloween', 'tobefonseca', '4.00', '2')\n"
     ]
    }
   ],
   "source": [
    "# Get artifact level features\n",
    "# For each design, get title, author, average score, number of scores, challenge name\n",
    "\n",
    "rows=[]\n",
    "\n",
    "for i in designs[:10]:\n",
    "    try:\n",
    "        url=\"https://www.threadless.com\"+i\n",
    "        response=requests.get(url)\n",
    "        soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # initiate the variable for each period\n",
    "        title=None\n",
    "        author=None\n",
    "        avg_score=None\n",
    "        total_score=None\n",
    "        \n",
    "        ##title\n",
    "        title=soup.select('div.submission-title h1')\n",
    "        if title!=[]:\n",
    "            title=title[0].text\n",
    "\n",
    "        ##author\n",
    "        author=soup.select('div.author-block a.author')\n",
    "        if author!=[]:\n",
    "            author=author[0].text\n",
    "\n",
    "        ##score\n",
    "        avg_score=soup.select('div.vote-avg span')\n",
    "        if avg_score!=[]:\n",
    "            avg_score=avg_score[0].text\n",
    "\n",
    "        ##total scores\n",
    "        total_score=soup.select('div.vote-count span')\n",
    "        if total_score!=[]:\n",
    "            total_score=total_score[0].text\n",
    "        \n",
    "        rows.append((title, author, avg_score, total_score))\n",
    "        print((title, author, avg_score, total_score))\n",
    "    \n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threadless\n",
      " 123012 designs\n",
      "\n",
      "Horror\n",
      " 4011 designs\n",
      "\n",
      "Video Games That Donâ€™t Exist Part 2., Presented by DesignerCon\n",
      " 215 designs\n",
      "\n",
      "Threadless\n",
      " 123012 designs\n",
      "\n",
      "Goblincore\n",
      " 356 designs\n"
     ]
    }
   ],
   "source": [
    "# Question: How to scrape the challenge information?\n",
    "\n",
    "# 1. challenge name\n",
    "# 2. how many designs per challenge\n",
    "\n",
    "\n",
    "# add your code here\n",
    "\n",
    "for i in designs[:5]:\n",
    "    \n",
    "    url=\"https://www.threadless.com\"+i\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "    challenge=soup.find(\"article\",class_=\"about-the-challenge\")\n",
    "    title=challenge.select(\"li.challenge-title\")[0].text\n",
    "    num=challenge.select(\"i.fa-thumbs-up\")[0].next_sibling.next_sibling.text\n",
    "    print(title, num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['moviereplicars', 'muktiharjanto', 'NoMotiveInc', 'rteestyle', 'Xentee', 'PammyB', 'leeagosila', 'kimprut', 'bakhus', 'gerberaloka', 'Astrid13', 'mhs23', 'CruzArtGallery', 'MitsukiRising', 'Gabriel0', 'KTRK', 'biernatt', 'SpaceDat120', 'churrumiaus', 'EdyArtSpace', 'ClosetPrints', 'Turborat14', 'CallyRaphael', 'i997', 'makerofstuff', 'UnfoundedHope', 'TheWestwood', 'santokie', 'tobefonseca', 'ArtofPig18', 'gulayfather', 'Elaraslove', 'monstiker', 'dragongrr', 'sunsetandchill', 'EmeraldMakes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get authors\n",
    "authors=[row[1] for row in rows]\n",
    "authors=filter(None, authors)\n",
    "authors_unique=list(set(authors))\n",
    "print(authors_unique)\n",
    "len(authors_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, '34 designs submitted', '339 designs scored', 'Avg Score Given: 3.87', 'Member since 2022', 'ClosetPrints']\n",
      "[None, '16 designs submitted', '6 designs scored', 'Avg Score Given: 5.00', 'Member since 2019', 'bakhus']\n",
      "[None, '162 designs submitted', '126 designs scored', 'Avg Score Given: 4.76', 'Member since 2022', 'muktiharjanto']\n",
      "[None, '9 designs submitted', '3 designs scored', 'Avg Score Given: 3.67', 'Member since 2022', 'CallyRaphael']\n",
      "[None, '12 designs submitted', '1 design scored', 'Avg Score Given: 5.00', 'Member since 2015', 'i997']\n"
     ]
    }
   ],
   "source": [
    "# For the designers we found, get the summary of their experience\n",
    "full=[]\n",
    "\n",
    "for i in authors_unique[:5]:\n",
    "    url=\"https://www.threadless.com/@\"+i\n",
    "    time.sleep(5)\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # find all stats\n",
    "    stats=soup.select('div.stats ul')\n",
    "    li=stats[0].find_all('li')\n",
    "    \n",
    "    line=[None] * 5\n",
    "    for j in li:\n",
    "        char=(j.text).strip()\n",
    "        \n",
    "        # threads\n",
    "        if re.search(\"started\",char):\n",
    "            line[0]=char\n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "            \n",
    "        # submitted\n",
    "        if re.search(\"submitted\",char):\n",
    "            line[1]=char   \n",
    "            #line[1]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # scored\n",
    "        if re.search(\"scored\",char):\n",
    "            line[2]=char\n",
    "            #line[2]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "        \n",
    "        # given\n",
    "        if re.search(\"Given\",char):\n",
    "            line[3]=char\n",
    "            #line[3]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "\n",
    "        # since\n",
    "        if re.search(\"since\",char):\n",
    "            line[4]=char\n",
    "            #line[4]=re.findall(r\"[0-9.]+\", char)[0]\n",
    "    \n",
    "    line.append(i)\n",
    "    print(line)\n",
    "    full.append(line)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClosetPrints 16 30\n",
      "bakhus 5 25\n",
      "muktiharjanto 94 65\n",
      "CallyRaphael 1 1\n",
      "i997 0 29\n"
     ]
    }
   ],
   "source": [
    "# Question: how to scrape each designers' numbers of followers and following?\n",
    "\n",
    "\n",
    "\n",
    "# add you code here\n",
    "for i in authors_unique[:5]:\n",
    "    url=\"https://www.threadless.com/@\"+i\n",
    "    time.sleep(5)\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "    \n",
    "    # get the section\n",
    "    follow=soup.select(\"div.following li\")\n",
    "    following=follow[0].select(\"a span\")[0].text\n",
    "    follower=follow[1].select(\"a span\")[0].text\n",
    "\n",
    "    print(i, following, follower)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the follower-followee network for each designer.\n",
    "# Can we do this with beautifulsoup? \n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['makerofstuff', 'Xentee']\n"
     ]
    }
   ],
   "source": [
    "relations=[]\n",
    "\n",
    "for i in authors_unique[3:5]:\n",
    "    \n",
    "    i=i.replace(\" \",\"%20\")\n",
    "    \n",
    "    follower_url=\"https://www.threadless.com/@\"+i+\"/followers\"\n",
    "    following_url=\"https://www.threadless.com/@\"+i+\"/following\"\n",
    "\n",
    "    # close a pop ad\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"user-agent=gene\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    # one's follower   \n",
    "    driver.get(follower_url)  \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # you can scroll many times if not reaching the end\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  \n",
    "    time.sleep(10)        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one's followers send the following tie\n",
    "            line=[name, i]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    \n",
    "    # one's follwing\n",
    "    driver.get(following_url)\n",
    "    time.sleep(10)   \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")                \n",
    "    time.sleep(25)  \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html.encode('utf-8'),\"html.parser\")\n",
    "    comp=soup.find('ol',class_=\"following-list\")\n",
    "    comp=comp.find_all(\"li\")\n",
    "\n",
    "    line=[]\n",
    "    for k in comp:\n",
    "        name=k.find(\"a\")[\"href\"]\n",
    "        name=name.lstrip(\"/@\")\n",
    "        if name in authors_unique:\n",
    "            # one sends the following tie to those to follow\n",
    "            line=[i, name]\n",
    "            print(line)\n",
    "            relations.append(line)\n",
    "    driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
